{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import time\n",
    "# from pathlib import Path\n",
    "# from copy import copy, deepcopy\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import scipy as sp\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "# from dominoes import leagueManager as lm\n",
    "# from dominoes import gameplay as dg\n",
    "# from dominoes import agents as da\n",
    "# from dominoes import utils\n",
    "# from dominoes import files as fm\n",
    "# from dominoes.networks import transformers as transformers\n",
    "# from dominoes import datasets\n",
    "# from dominoes import training\n",
    "# from dominoes.analysis import transformer_analysis as ta\n",
    "# from dominoes import utils\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# highestDominoe = 9\n",
    "# numPlayers = 4\n",
    "# game = dg.dominoeGame(highestDominoe, numPlayers=numPlayers) \n",
    "# game.playGame(rounds=500) # Play the game \n",
    "# game.printResults(fullScore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.gameSequenceToString(game.dominoes, game.lineSequence, game.linePlayDirection, player=None, playNumber=None, labelLines=True)\n",
    "# utils.gameSequenceToString(game.dominoes, game.dummySequence, game.dummyPlayDirection, player=None, playNumber=None, labelLines=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.ndimage import median_filter\n",
    "# from scipy.stats import ttest_rel\n",
    "\n",
    "# # general variables for experiment\n",
    "# POINTER_METHODS = ['PointerStandard', 'PointerDot', 'PointerDotLean', 'PointerDotNoLN', 'PointerAttention', 'PointerTransformer']\n",
    "# numNets = len(POINTER_METHODS)\n",
    "\n",
    "# # method for returning the name of the saved network parameters (different save for each possible opponent)\n",
    "# def getFileName(baseName, extra=None):\n",
    "#     if extra is not None:\n",
    "#         baseName = baseName + f\"_{extra}\"\n",
    "#     return baseName\n",
    "\n",
    "# pretrained_method = 'PointerStandard'\n",
    "\n",
    "# # Load previously stored settings and pretrained networks \n",
    "# baseName = 'ptrArchComp_sequencer_RL_withBaseline'\n",
    "# results, args = utils.loadSavedExperiment(fm.prmPath(), fm.resPath(), getFileName(baseName))\n",
    "# nets = [torch.load(fm.netPath() / getFileName(baseName, extra=f\"{pretrained_method}.pt\")) for _ in POINTER_METHODS]\n",
    "# for net, method in zip(nets, POINTER_METHODS):\n",
    "#     net.pointer_method = method\n",
    "#     net.pointer.pointer_method = method\n",
    "#     # build pointer (chooses an output)\n",
    "#     if method == 'PointerStandard':\n",
    "#         # output of the network uses a pointer attention layer as described in the original paper\n",
    "#         net.pointer.pointer = transformers.PointerStandard(net.pointer.embedding_dim)\n",
    "\n",
    "#     elif method == 'PointerDot':\n",
    "#         net.pointer.pointer = transformers.PointerDot(net.pointer.embedding_dim)\n",
    "\n",
    "#     elif method == 'PointerDotNoLN':\n",
    "#         net.pointer.pointer = transformers.PointerDotNoLN(net.pointer.embedding_dim)\n",
    "\n",
    "#     elif method == 'PointerDotLean':\n",
    "#         net.pointer.pointer = transformers.PointerDotLean(net.pointer.embedding_dim)\n",
    "        \n",
    "#     elif method == 'PointerAttention':\n",
    "#         kwargs = {'heads':net.heads, 'kqnorm':net.kqnorm}\n",
    "#         net.pointer.pointer = transformers.PointerAttention(net.pointer.embedding_dim, **kwargs)\n",
    "\n",
    "#     elif method == 'PointerTransformer':\n",
    "#         kwargs = {'heads':net.heads, 'expansion':1, 'kqnorm':net.kqnorm, 'bias':net.bias}\n",
    "#         net.pointer.pointer = transformers.PointerTransformer(net.pointer.embedding_dim, **kwargs)\n",
    "        \n",
    "#     else:\n",
    "#         raise ValueError(f\"the pointer_method was not set correctly, {args.pointer_layer} not recognized\")\n",
    "# nets = [net.to(device) for net in nets]\n",
    "\n",
    "# # Define parameters to be learned and turn off gradients in all other parameters\n",
    "# learning_parameters = [[] for _ in nets]\n",
    "# for learn, net in zip(learning_parameters, nets):\n",
    "#     for name, prm in net.named_parameters():\n",
    "#         if 'pointer.pointer' in name:\n",
    "#             prm.requires_grad = True\n",
    "#             learn.append(prm)\n",
    "#         else:\n",
    "#             prm.requires_grad = False\n",
    "\n",
    "# # Prepare optimizer for the specific parameters to be learned\n",
    "# optimizers = [torch.optim.Adam(learn, lr=1e-3, weight_decay=1e-5) for learn in learning_parameters]\n",
    "\n",
    "\n",
    "# def resetBaselines(blnets, batchSize, highestDominoe, listDominoes, handSize, num_output, value_method, **kwargs):\n",
    "#     # initialize baseline input (to prevent overfitting with training data)\n",
    "#     batch = datasets.generateBatch(highestDominoe, listDominoes, batchSize, handSize, **kwargs)\n",
    "#     baselineinput, _, _, _, _, baseline_selection, baseline_available = batch \n",
    "#     baselineinput = baselineinput.to(device) # move to device\n",
    "\n",
    "#     # divide input into main input and context\n",
    "#     baseline_x, baseline_context = baselineinput[:, :-1].contiguous(), baselineinput[:, [-1]] # input [:, [-1]] is context token\n",
    "#     baseline_input = (baseline_x, baseline_context)\n",
    "\n",
    "#     _, baseline_choices = map(list, zip(*[net(baseline_input, max_output=num_output) for net in blnets]))\n",
    "\n",
    "#     # measure rewards for each sequence\n",
    "#     baseline_rewards = [training.measureReward_sequencer(baseline_available, listDominoes[baseline_selection], choice, value_method=value_method, normalize=False)\n",
    "#                 for choice in baseline_choices]\n",
    "\n",
    "#     return baseline_input, baseline_selection, baseline_available, baseline_rewards \n",
    "\n",
    "\n",
    "# def get_gamma_transform(gamma, N):\n",
    "#     exponent = torch.arange(N).view(-1,1) - torch.arange(N).view(1,-1)\n",
    "#     gamma_transform = (gamma ** exponent * (exponent >= 0))\n",
    "#     return gamma_transform\n",
    "\n",
    "\n",
    "# # get values from the argument parser\n",
    "# highestDominoe = args.highest_dominoe\n",
    "# listDominoes = utils.listDominoes(highestDominoe)\n",
    "\n",
    "# handSize = args.hand_size\n",
    "# batchSize = args.batch_size\n",
    "# null_token = True # using a null token to indicate end of line\n",
    "# null_index = copy(handSize) # index of null token\n",
    "# available_token = True # using available token to indicate which value to start on \n",
    "# ignore_index = -1\n",
    "# value_method = '1' # method for generating rewards in reward function\n",
    "\n",
    "# num_output = copy(handSize)\n",
    "# gamma = args.gamma\n",
    "# gamma_transform = get_gamma_transform(gamma, num_output).to(device)\n",
    "\n",
    "# # other batch parameters\n",
    "# batchSize = args.batch_size\n",
    "# baselineBatchSize = args.baseline_batch_size\n",
    "# significance = args.significance\n",
    "# do_baseline = not(args.nobaseline)\n",
    "\n",
    "# # network parameters\n",
    "# temperature = args.temperature\n",
    "\n",
    "# # train parameters\n",
    "# trainEpochs = 8000 #args.train_epochs\n",
    "# testEpochs = 100 #args.test_epochs\n",
    "\n",
    "# print(f\"Doing training...\")\n",
    "# trainReward = torch.zeros((trainEpochs, numNets))\n",
    "# testReward = torch.zeros((testEpochs, numNets))\n",
    "# testEachReward = torch.zeros((testEpochs, numNets, batchSize))\n",
    "# testMaxReward = torch.zeros((testEpochs, batchSize))\n",
    "\n",
    "\n",
    "# if do_baseline:\n",
    "#     # create baseline nets, initialized as copy of learning nets\n",
    "#     blnets = [deepcopy(net) for net in nets]\n",
    "#     for blnet in blnets:\n",
    "#         blnet.setTemperature(1.0)\n",
    "#         blnet.setThompson(True)\n",
    "\n",
    "#     baseline_kwargs = dict(return_target=False, null_token=null_token, available_token=available_token, \n",
    "#                             ignore_index=ignore_index, return_full=True)            \n",
    "#     baseline_data = resetBaselines(blnets, baselineBatchSize, highestDominoe, listDominoes, handSize, num_output, value_method, **baseline_kwargs)\n",
    "#     baseline_input, baseline_selection, baseline_available, baseline_rewards = baseline_data\n",
    "    \n",
    "\n",
    "# for epoch in tqdm(range(trainEpochs)):\n",
    "#     # generate input batch\n",
    "#     batch = datasets.generateBatch(highestDominoe, listDominoes, batchSize, handSize, return_target=False, null_token=null_token,\n",
    "#                                 available_token=available_token, ignore_index=ignore_index, return_full=True)\n",
    "\n",
    "#     # unpack batch tuple\n",
    "#     input, _, _, _, _, selection, available = batch\n",
    "\n",
    "#     # move to correct device\n",
    "#     input = input.to(device)\n",
    "\n",
    "#     # divide input into main input and context\n",
    "#     x, context = input[:, :-1].contiguous(), input[:, [-1]] # input [:, [-1]] is context token\n",
    "#     input = (x, context)\n",
    "    \n",
    "#     # zero gradients, get output of network\n",
    "#     for opt in optimizers: opt.zero_grad()\n",
    "#     log_scores, choices = map(list, zip(*[net(input, max_output=num_output) for net in nets]))\n",
    "\n",
    "#     # measure rewards for each sequence\n",
    "#     rewards = [training.measureReward_sequencer(available, listDominoes[selection], choice, value_method=value_method, normalize=False)\n",
    "#                 for choice in choices]\n",
    "#     G = [torch.matmul(reward, gamma_transform) for reward in rewards]\n",
    "#     logprob_policy = [torch.gather(score, 2, choice.unsqueeze(2)).squeeze(2)\n",
    "#                         for score, choice in zip(log_scores, choices)] # log-probability for each chosen dominoe\n",
    "    \n",
    "#     if do_baseline:\n",
    "#         # - do \"baseline rollout\" with baseline networks - \n",
    "#         with torch.no_grad():\n",
    "#             _, bl_choices = map(list, zip(*[net(input, max_output=num_output) for net in blnets]))\n",
    "#             bl_rewards = [training.measureReward_sequencer(available, listDominoes[selection], choice, value_method=value_method, normalize=False)\n",
    "#                             for choice in bl_choices]\n",
    "#             bl_G = [torch.matmul(reward, gamma_transform) for reward in bl_rewards]\n",
    "#             adjusted_G = [g-blg for g, blg in zip(G, bl_G)] # baseline corrected G\n",
    "#     else:\n",
    "#         # for a consistent namespace\n",
    "#         adjusted_G = [g for g in G]\n",
    "\n",
    "#     # do backward pass on J and update networks\n",
    "#     for policy, g, opt in zip(logprob_policy, adjusted_G, optimizers):\n",
    "#         J = -torch.sum(policy * g)\n",
    "#         J.backward()\n",
    "#         opt.step()\n",
    "\n",
    "#     # save training data\n",
    "#     for i, reward in enumerate(rewards):\n",
    "#         trainReward[epoch, i] = torch.mean(torch.sum(reward, dim=1))\n",
    "\n",
    "    \n",
    "#     # check if we should update baseline networks\n",
    "#     if do_baseline:\n",
    "#         with torch.no_grad():\n",
    "#             # first measure policy on baseline data (in evaluation mode)\n",
    "#             for net in nets: \n",
    "#                 net.setTemperature(1.0)\n",
    "#                 net.setThompson(False)\n",
    "\n",
    "#             _, choices = map(list, zip(*[net(baseline_input, max_output=num_output) for net in nets]))\n",
    "#             rewards = [training.measureReward_sequencer(baseline_available, listDominoes[baseline_selection], choice, value_method=value_method, normalize=False)\n",
    "#                         for choice in choices]\n",
    "            \n",
    "#             _, p = map(list, zip(*[ttest_rel(r.view(-1).cpu().numpy(), blr.view(-1).cpu().numpy(), alternative='greater')\n",
    "#                                     for r, blr in zip(rewards, baseline_rewards)]))\n",
    "#             do_update = [pv<significance for pv in p]\n",
    "\n",
    "#             # for any networks with significantly different values, update them\n",
    "#             for ii, update in enumerate(do_update):\n",
    "#                 if update:\n",
    "#                     blnets[ii] = deepcopy(nets[ii])\n",
    "#                     blnets[ii].setTemperature(1.0)\n",
    "#                     blnets[ii].setThompson(False)\n",
    "\n",
    "#             # regenerate baseline data and get baseline network policy\n",
    "#             if any(do_update):\n",
    "#                 baseline_data = resetBaselines(blnets, baselineBatchSize, highestDominoe, listDominoes, handSize, num_output, value_method, **baseline_kwargs)\n",
    "#                 baseline_input, baseline_selection, baseline_available, baseline_rewards = baseline_data\n",
    "                \n",
    "#             # return nets to training state\n",
    "#             for net in nets: \n",
    "#                 net.setTemperature(temperature) \n",
    "#                 net.setThompson(True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     print('Testing network...')\n",
    "#     for net in nets:\n",
    "#         net.setTemperature(1.0)\n",
    "#         net.setThompson(False)\n",
    "\n",
    "#     for epoch in tqdm(range(testEpochs)):\n",
    "#         # generate input batch\n",
    "#         batch = datasets.generateBatch(highestDominoe, listDominoes, batchSize, handSize, return_target=True, null_token=null_token,\n",
    "#                                     available_token=available_token, ignore_index=ignore_index, return_full=True, value_method='length')\n",
    "\n",
    "#         # unpack batch tuple\n",
    "#         input, target, _, _, _, selection, available = batch\n",
    "#         assert torch.all(torch.sum(target==null_index, dim=1)==1), \"null index is present more or less than once in at least one target\"\n",
    "\n",
    "#         # move to correct device\n",
    "#         input, target = input.to(device), target.to(device)\n",
    "#         target = target[:, :num_output].contiguous()\n",
    "#         target[target==ignore_index]=null_index # need to convert this to a valid index for measuring reward of target\n",
    "\n",
    "#         # divide input into main input and context\n",
    "#         x, context = input[:, :-1].contiguous(), input[:, [-1]] # input [:, [-1]] is context token\n",
    "#         input = (x, context)\n",
    "        \n",
    "#         log_scores, choices = map(list, zip(*[net(input, max_output=num_output) for net in nets]))\n",
    "\n",
    "#         # measure rewards for each sequence\n",
    "#         rewards = [training.measureReward_sequencer(available, listDominoes[selection], choice, value_method=value_method, normalize=False)\n",
    "#                 for choice in choices]\n",
    "        \n",
    "#         # save testing data\n",
    "#         for i, reward in enumerate(rewards):\n",
    "#             testReward[epoch, i] = torch.mean(torch.sum(reward, dim=1))\n",
    "#             testEachReward[epoch, i] = torch.sum(reward, dim=1)\n",
    "\n",
    "#         # measure rewards for target (defined as longest possible sequence of the dominoes in the batch\n",
    "#         target_reward = training.measureReward_sequencer(available, listDominoes[selection], target, value_method=value_method, normalize=False)\n",
    "#         testMaxReward[epoch] = torch.sum(target_reward, dim=1)\n",
    "            \n",
    "        \n",
    "# results = {\n",
    "#     'trainReward': trainReward,\n",
    "#     'testReward': testReward,\n",
    "#     'testEachReward': testEachReward,\n",
    "#     'testMaxReward': testMaxReward,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap = mpl.colormaps['tab10']\n",
    "\n",
    "# # Process test results in comparison to maximum possible\n",
    "# minMaxReward = torch.min(results['testMaxReward'])\n",
    "# maxMaxReward = torch.max(results['testMaxReward'])\n",
    "# uniqueRewards = torch.arange(minMaxReward, maxMaxReward+1)\n",
    "# numUnique = len(uniqueRewards)\n",
    "# rewPerMax = torch.zeros((len(POINTER_METHODS), numUnique))\n",
    "# for iur, ur in enumerate(uniqueRewards):\n",
    "#     idx_ur = results['testMaxReward']==ur\n",
    "#     for ii, name in enumerate(POINTER_METHODS):\n",
    "#         rewPerMax[ii, iur] = torch.mean(results['testEachReward'][:, ii][idx_ur])\n",
    "            \n",
    "# # make plot of performance trajectory\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(6, 4), width_ratios=[2.6, 1], layout=\"constrained\")\n",
    "# for idx, name in enumerate(POINTER_METHODS):\n",
    "#     adata = median_filter(results['trainReward'][:,idx], size=(10,))\n",
    "#     ax[0].plot(range(trainEpochs), adata, color=cmap(idx), lw=1.2, label=name)\n",
    "# ax[0].set_ylabel(f'Total Reward')\n",
    "# ax[0].set_title('Training Performance')\n",
    "# ax[0].legend(loc='best', fontsize=9)\n",
    "# # ax[0].set_xticks([0, 2500, 5000, 7500, 10000])\n",
    "# ylims = ax[0].get_ylim()\n",
    "\n",
    "# xOffset = [-0.2, 0.2]\n",
    "# get_x = lambda idx: [xOffset[0]+idx, xOffset[1]+idx]\n",
    "# for idx, name in enumerate(POINTER_METHODS):\n",
    "#     mnTestReward = torch.mean(results['testReward'][:,idx], dim=0)\n",
    "#     ax[1].plot(get_x(idx), [mnTestReward, mnTestReward], color=cmap(idx), lw=4, label=name)\n",
    "# ax[1].set_xticks(range(len(POINTER_METHODS)))\n",
    "# ax[1].set_xticklabels([pmethod[7:] for pmethod in POINTER_METHODS], rotation=45, ha='right', fontsize=8)\n",
    "# ax[1].set_ylabel(f'Reward')\n",
    "# ax[1].set_title('Testing')\n",
    "# ax[1].set_xlim(-1, len(POINTER_METHODS))\n",
    "# # ax[1].set_ylim(ylims)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # Plot rewards in comparison to maximum possible for each network type\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(6, 4), layout='constrained')\n",
    "# for idx, name in enumerate(POINTER_METHODS):\n",
    "#     adata = rewPerMax[idx]\n",
    "#     ax.plot(uniqueRewards, adata, color=cmap(idx), lw=1.2, marker='o', markersize=4, label=name)\n",
    "# ax.plot(uniqueRewards, uniqueRewards, color='k', lw=1.2, linestyle='--', label='max possible')\n",
    "# ax.set_ylim(0, max(uniqueRewards)+1)\n",
    "# ax.set_xticks(uniqueRewards)\n",
    "# ax.set_xlabel('Maximum Possible Reward')\n",
    "# ax.set_ylabel('Actual Reward Acquired')\n",
    "# ax.legend(loc='upper left', fontsize=10)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POINTER_METHODS = ta.POINTER_METHODS\n",
    "# net_lookup = {val:idx for idx,val in enumerate(POINTER_METHODS)}\n",
    "# netidx = lambda x: net_lookup[x]\n",
    "\n",
    "# batchSize = 128\n",
    "# num_cities = 8\n",
    "# baseName = \"ptrArchComp_TSP_RL\"\n",
    "# results, nets = ta.loadNetworks(baseName)\n",
    "# outs = ta.process_tsp_data(nets, batchSize, num_cities)\n",
    "# embedded, encoded, intermediate, decoder_context, decoder_input, scores, choices, batch = outs\n",
    "# _, xy, dists = batch # don't need the input representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MDS for embedded vs positions\n",
    "# idx = netidx('PointerDot')\n",
    "# dist_method = ta.torch_pdist\n",
    "# dist_data = dist_method(xy.view(batchSize*num_cities, -1))\n",
    "# dist_embedded = dist_method(embedded[idx].view(batchSize*num_cities, -1))\n",
    "# dist_encoded = dist_method(encoded[idx].view(batchSize*num_cities, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_mats = [dist_data, dist_embedded, dist_encoded]\n",
    "# order_data = [ta.compute_serial_matrix(dm.cpu().numpy())[1] for dm in dist_mats]\n",
    "# ser_data = [[ta.seriate_matrix(dm.cpu().numpy(), od) for dm in dist_mats] for od in order_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(3,3,figsize=(9,9), layout='constrained')\n",
    "# for ii in range(3):\n",
    "#     for jj in range(3):\n",
    "#         ax[ii,jj].imshow(ser_data[ii][jj])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
