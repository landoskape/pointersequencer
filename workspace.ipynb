{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d743f019-bcc5-4c13-8aef-98e737e0747e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import time\n",
    "# from pathlib import Path\n",
    "# from copy import copy, deepcopy\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import scipy as sp\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "# from dominoes import leagueManager as lm\n",
    "# from dominoes import gameplay as dg\n",
    "# from dominoes import agents as da\n",
    "# from dominoes import utils\n",
    "# from dominoes import files as fm\n",
    "# from dominoes.networks import transformers as transformers\n",
    "# from dominoes import datasets\n",
    "# from dominoes import training\n",
    "# from dominoes.analysis import transformer_analysis as ta\n",
    "# from dominoes import utils\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52362dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dominoes tasks:\n",
    "1. Plot valueAgentELO results effectively, document it\n",
    "2. Train a library of lineValueAgents on a server\n",
    "3. Build a policy-only agent\n",
    "   - learn network approximation of value function same way other agents do\n",
    "   - process hand and game state to generate a context embedding\n",
    "   - then use pointer networks to choose which option to play based on the option encoding and the context\n",
    "   - policy updated with temporal-difference of the value function\n",
    "4. Build a win-probability agent method (to be applied to all agents) \n",
    "   - I think this is going to involve a bit of cleaning and refactoring of my agents code\n",
    "\n",
    "Dominoes Experiments:\n",
    "1. Compare lineValueNetwork trained on only it's turn vs. all turns. vs. it's turn and next turn\n",
    "2. Compare value agents trained with hand-continuity of eligibility traces or resets\n",
    "\n",
    "Dominoes Analysis Goals:\n",
    "1. Study some of the key layers of agent value networks!\n",
    "2. Initialize the game many times and correalte lineValueFeatures with final score output\n",
    "\n",
    "Dominoes Coding Practice:\n",
    "1. Still need to refactor some of the \"experiment\" scripts to make sure they divide results and plotting \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Pointer Immediate Tasks:\n",
    "1. Use a separate value network to force encoding to predict the total rewards of the pointer output\n",
    "2. Run an experiment where I take trained pointer networks and transition them to a dominoe based value \n",
    "function and a gamma < 1, and show that they can learn to prioritize playing high value dominoes first. Then...\n",
    "3. Add the context vector that encodes the number of turns left (with uncertainty?)\n",
    "    - so the full pointer network will get an extra context input that describes how many turns are left\n",
    "    - 0 rewards will be given after the possible turns are over\n",
    "    - so the network will have to learn to get as much value out as quickly as possible\n",
    "4. Also apply these networks to the vehicle routing problem?\n",
    "5. Analyze encoding space of pointer networks...\n",
    "6. Do the encoder swap of different pointer layers...\n",
    "7. Does the speed of learning for the different networks on the sequencer task come from true performance or just sensitivity to the temperature? \n",
    "\n",
    "Pointer Big Ideas:\n",
    "1. Develop new tasks based on graphs, random forests, and complex rules and determine whether the sophisticated \n",
    "pointer layer architecture works better on them. \n",
    "\n",
    "Pointer Coding Practice:\n",
    "1. Convert reward functions to a class \n",
    "2. Convert dataset preparation to a class\n",
    "\n",
    "Pointer Mechanical Update:\n",
    "1. Change it to learn an initial decoder to be used whenever an input position isn't provided! \n",
    "   - self.custom_tensor = nn.Parameter(torch.randn(input_size, output_size), requires_grad=True)\n",
    "2. Add mechanism for storing hidden parameters to entire pointer network\n",
    "\"\"\"\n",
    "\n",
    "# THE CODE IS PRETTY UNORGANIZED :(\n",
    "   # need to organize the dominoes agents better (especially with regards to whether they predict their final score or the win probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c715a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO For refactoring\n",
    "# at the moment there is no flexibility on the loss function control for the supervised dataset!!!!\n",
    "\n",
    "# Add mechanism for printing the arguments used to build a pointernetwork so the user can see what they did. \n",
    "\n",
    "# Add documentation of baseline updates and performance etc\n",
    "# Add some dataset specific summary plots and integrate into plotting code? \n",
    "# Get the supervised learning methods working for each dataset and task\n",
    "# Checkpointing, figure making, logging, etc\n",
    "\n",
    "# it worked!!! now trying without embedding bias...\n",
    "# it works without embedding bias. It works (with different speeds per pointer layer!) with lower train temperature\n",
    " # (but of course that could be because of differential sensitivity to temperature..., should test that directly)\n",
    "# now trying with 1 encoding layer. \n",
    "\n",
    "# :)\n",
    "\n",
    "\n",
    "# TODO: \n",
    "# DOMINOES SEQUENCER Comparison of max to real reward:\n",
    "# - Add target to batch (can do post-hoc, even if not requested)\n",
    "# - Measure reward of target\n",
    "# - Add a 2D vector comparing max and real reward for each batch element!!\n",
    "\n",
    "# TSP Distance Traveled:\n",
    "# - Explicitly measure the distance traveled by the agent in the TSP task\n",
    "# - Compare to Held-Karp Solution\n",
    "\n",
    "\n",
    "# TODO ASAP!!!!!!\n",
    "# - Get test result plots in there for good plotting, then start running experiments with different parameters\n",
    "#   so you can save / see the results!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8404c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from time import time\n",
    "import torch\n",
    "from dominoes.datasets import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78fe2eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "dists\n",
      "init\n",
      "num_cities\n",
      "coord_dims\n",
      "batch_size\n",
      "return_target\n",
      "ignore_index\n",
      "threads\n",
      "target\n"
     ]
    }
   ],
   "source": [
    "task = \"tsp\"\n",
    "dataset = get_dataset(task, build=True, num_cities=10)\n",
    "batch = dataset.generate_batch(batch_size=4, return_target=True)\n",
    "\n",
    "for k in batch.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b2ff400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 7, 4, 2, 5, 6, 1, 0, 8, 3],\n",
       "        [1, 9, 5, 6, 2, 0, 4, 8, 3, 7],\n",
       "        [6, 3, 1, 9, 2, 0, 7, 5, 8, 4],\n",
       "        [7, 5, 3, 9, 1, 6, 8, 2, 4, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82f70f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 7 to 6 with reward 0.15, distance: 0.15\n",
      "From 6 to 9 with reward 0.32, distance: 0.32\n",
      "From 9 to 0 with reward 0.34, distance: 0.34\n",
      "From 0 to 5 with reward 0.57, distance: 0.57\n",
      "From 5 to 8 with reward 0.29, distance: 0.29\n",
      "From 8 to 3 with reward 0.28, distance: 0.28\n",
      "From 3 to 2 with reward 0.56, distance: 0.56\n",
      "From 2 to 4 with reward 0.48, distance: 0.48\n",
      "From 4 to 1 with reward 0.40, distance: 0.40\n"
     ]
    }
   ],
   "source": [
    "reward = dataset.reward_function(batch[\"target\"], batch)\n",
    "\n",
    "ib = 0\n",
    "prev = batch[\"init\"][ib]\n",
    "for i, s in enumerate(batch[\"target\"][ib]):\n",
    "    if i==(batch[\"target\"].size(1)-1):\n",
    "        add_dist = batch[\"dists\"][ib][s, batch[\"init\"][ib]]\n",
    "    else:\n",
    "        add_dist = 0\n",
    "    print(f\"From {prev} to {s} with reward {-reward[ib][i]:.2f}, distance: {batch['dists'][ib][prev, s] + add_dist :.2f}\")\n",
    "    prev = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865fb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b81df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "highest_dominoe = 9\n",
    "dataset = DominoeDataset(\"sequencer\", highest_dominoe, hand_size=8, return_target=True)\n",
    "\n",
    "batch = dataset.generate_batch(train=False, batch_size=4, value_method=\"length\")\n",
    "dominoes = dataset.get_dominoe_set(train=False)\n",
    "\n",
    "target_as_choice = batch[\"target\"].clone()\n",
    "target_as_choice[target_as_choice==-1] = dataset.prms[\"hand_size\"]\n",
    "\n",
    "reward, direction = dataset._measurereward_sequencer(target_as_choice, batch, return_direction=True)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80eea3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n",
      "tensor([6., 8.]) reverse\n",
      "tensor([2., 6.]) reverse\n",
      "tensor([2., 8.]) forward\n",
      "tensor([1., 8.]) reverse\n",
      "tensor([1., 4.]) forward\n",
      "tensor([-1., -1.]) reverse\n",
      "tensor([-1., -1.]) reverse\n",
      "tensor([-1., -1.]) reverse\n",
      "tensor([-1., -1.]) reverse\n"
     ]
    }
   ],
   "source": [
    "ib = 1\n",
    "hand = dominoes[batch[\"selection\"][ib]]\n",
    "hand_null = torch.cat([hand, -torch.ones((1, 2))], dim=0)\n",
    "\n",
    "print(batch[\"available\"][ib])\n",
    "for c, d in zip(target_as_choice[ib], direction[ib]):\n",
    "    print(hand_null[c], \"forward\" if d==0 else \"reverse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
